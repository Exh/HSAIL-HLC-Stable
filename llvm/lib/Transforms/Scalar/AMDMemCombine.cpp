#if 1 || defined (AMD_OPENCL_MEMCOMBINE)
//===- AMDMemCombine.cpp - Vectorziess memory accesses ---------------===//
//===----------------------------------------------------------------------===//
//
// This file will implement a version of memory vectorization similar to SLP
// or Superword-Level Parallelism 
// 
//===----------------------------------------------------------------------===//

#define DEBUG_TYPE "memcombine"
#include "llvm/DataLayout.h"
#include "llvm/Function.h"
#include "llvm/LLVMContext.h"
#include "llvm/Pass.h"
//#include "llvm/ADT/Statistic.h"
#include "llvm/ADT/ValueMap.h"
#include "llvm/AMDLLVMContextHook.h"
#include "llvm/Analysis/AMDAlignmentAnalysis.h"
#include "llvm/Analysis/MemoryDependenceAnalysis.h"
#include "llvm/Analysis/ValueTracking.h"
#include "llvm/Analysis/AMDOpenCLSymbols.h"
#include "llvm/Constants.h"
#include "llvm/Transforms/Utils/BasicBlockUtils.h"
#include "llvm/Transforms/Scalar.h"
#include "llvm/Support/CommandLine.h"
#include "llvm/Support/Debug.h"
#include "llvm/Support/raw_ostream.h"
#include <set>
#include <sstream>
#include <stack>

using namespace llvm;

// options for "opt" driver
static cl::opt<bool>
VectorizeLoads(
    "vectorize-loads", cl::init(true),
    cl::value_desc("true/false"),
    cl::desc("controls vectorization of loads in AMDMemCombine"));

static cl::opt<bool>
VectorizeStores(
    "vectorize-stores", cl::init(true),
    cl::value_desc("true/false"),
    cl::desc("controls vectorization of stores in AMDMemCombine"));

static cl::opt<int>
MemCombineMaxVecGen(
    "memcombine-max-vec-gen", cl::init(1),
    cl::value_desc("bytes"),
    cl::desc("Maximum width of vector loads/stores generated by memory access"
             " combining."));

static cl::opt<int>
MemCombineMinAlign(
    "memcombine-min-align", cl::init(4),
    cl::value_desc("bytes"),
    cl::desc("Mininum alignment if smaller than load/store size"));

#if 0
STATISTIC(NumVecLoads, "# of vector loads placed");
STATISTIC(NumVecStores, "# of vector stores placed");
STATISTIC(NumLDDeleted, "# of loads deleted due to vectorization");
STATISTIC(NumSTDeleted, "# of stores deleted due to vectorization");
#endif

static void DumpDecomposeGEPExpression(
  const Value *V, const Value* base,
  int64_t BaseOffs,
  SmallVectorImpl<VariableGEPIndex> &VarIndices);

namespace {
  class InstPosMapTy;

  // Map from an instruction to a integer position.
  class InstPosMapTy {
    typedef DenseMap<const Instruction*, unsigned> InstToIntMapTy;

  public:
    enum { BadPos = -1 };

  private:
    InstToIntMapTy Map;

  public:
    int getPos(const Instruction& inst) const {
      InstToIntMapTy::const_iterator it = Map.find(&inst);
      if (it == Map.end()) return BadPos;

      return (int)it->second;
    }

    unsigned& operator[](const Instruction* inst) {
      return Map[inst];
    }

    void clear() {
      Map.clear();
    }
  };

  typedef std::pair<Instruction*,Instruction*> InstPair;
  static const int64_t BadConstantOffset = (int64_t)(~(1LL << 63));

  // The main body of this pass
  class MemCombine: public BasicBlockPass {
    typedef SmallVector<Instruction*, 32> InstVecTy;
    typedef SmallVector<std::pair<Instruction*,Instruction*>, 32> InstPairVecTy;

  private:
    AlignmentAnalysis *AlignAnalysis;
    MemoryDependenceAnalysis *MD;
    const DataLayout *DL;
    // Heuristics: Max width (#bytes) of vector loads/stores to be generated
    unsigned MaxGeneratedVecWidth;
    // Heuristics: Minimum alignment of loads/stores required by hardware
    unsigned MinAlignment;

    // list of loads and stores in current BB that may potentially be
    // combined with other loads/stores
    InstVecTy CandLdSts;
    // pair of loads or stores that should be combined
    InstPairVecTy CandLdStPairs;
    // holds position indexing of each instruction in the region
    InstPosMapTy InstPosMap;

  private:
    virtual void getAnalysisUsage(AnalysisUsage &AU) const;

    int memInstInterferePos(Instruction& memInst);

    void collectLdSts(BasicBlock& BB);

    inline const Value* getLdStAddrExpr(
      const Instruction& inst, int64_t& offset,
      SmallVectorImpl<VariableGEPIndex>& varIndices);

    int64_t getAddrConstOffset(const Instruction& inst1,
                               const Instruction& inst2);

    inline bool inCandLdStPairs(const Instruction& inst);
    void findCandPairs();

    bool dominates(const Value& i1, const Value& i2);
    bool okToHoist(const Instruction *anchor, const Instruction *i);
    void hoist(Instruction *anchor, Instruction *i);
    CastInst* castBasePtr(Instruction& baseInst,
                          PointerType *pvt,
                          Instruction& insertBefore,
                          bool isLoad);
    void replaceLoadWithExtractElem(LoadInst& vecload,
                                    VectorType* vt,
                                    unsigned startIdx,
                                    LoadInst& load,
                                    Type* loadType);
    Value& replaceStoresWithShuffleVec(Type* type,
                                       InstPair storePair,
                                       Instruction& insertBefore);
    Value& replaceStoreWithInsertElem(StoreInst& store,
                                      Value* vec,
                                      size_t idx,
                                      Instruction& insertBefore);
    Value& replaceStoreWithShuffleVec(Type* vecType,
                                      size_t startIdx,
                                      StoreInst& store,
                                      Type* storeType,
                                      Instruction& insertBefore);
    Value& replaceStoresWithShuffleVecOrInsertElem(Type* type1,
                                                   Type* type2,
                                                   VectorType *vt,
                                                   InstPair storePair,
                                                   Instruction& insertBefore);
    VectorType* getCombinedVecType(Type* type1, Type* type2);
    bool combinePair(InstPair pair);
    bool combineMemSiblings(BasicBlock& BB);
  public:
    static char ID; // Pass identification, replacement for typeid
    MemCombine(unsigned maxVecGen = MemCombineMaxVecGen,
               unsigned minAlign = MemCombineMinAlign);

    virtual bool runOnBasicBlock(BasicBlock& BB);
    virtual ~MemCombine() {
      //errs() << "MemCombine: Destructor called\n"; 
    }
  };
}

char MemCombine::ID = 0;
INITIALIZE_PASS_BEGIN(MemCombine, "amdmemcombine", "AMD Memory vectorizer",
                      false, false);
INITIALIZE_AG_DEPENDENCY(AliasAnalysis);
INITIALIZE_AG_DEPENDENCY(AlignmentAnalysis);
INITIALIZE_PASS_DEPENDENCY(MemoryDependenceAnalysis);
INITIALIZE_PASS_END(MemCombine, "amdmemcombine", "AMD Memory vectorizer",
                      false, false);

// Returns the pointer operand of the given load or store instruction
static inline Value* getLdStPointerOperand(Instruction& inst)
{
  LoadInst* load = dyn_cast<LoadInst>(&inst);
  StoreInst* store = dyn_cast<StoreInst>(&inst);
  assert((load || store) && "not load or store");
  Value* addr = load ? load->getPointerOperand()
                     : store->getPointerOperand();
  return addr;
}

// Returns the pointer operand of the given load or store instruction
static inline const Value* getLdStPointerOperand(const Instruction& inst)
{
  const LoadInst* load = dyn_cast<LoadInst>(&inst);
  const StoreInst* store = dyn_cast<StoreInst>(&inst);
  assert((load || store) && "not load or store");
  const Value* addr = load ? load->getPointerOperand()
                           : store->getPointerOperand();
  return addr;
}

void MemCombine::getAnalysisUsage(AnalysisUsage &AU) const {
  AU.addRequired<AliasAnalysis>();
  AU.addRequired<MemoryDependenceAnalysis>();
  AU.addRequired<OpenCLSymbols>();

  AU.addPreserved<AliasAnalysis>();
  AU.addPreserved<OpenCLSymbols>();
  //AU.addPreserved<MemoryDependenceAnalysis>();
}

// Return the position of a mem inst's closest interference instruction
int MemCombine::memInstInterferePos(Instruction& memInst)
{
  Instruction* inst = &memInst;
  Instruction* depInst = NULL;
  bool IsLoad = isa<LoadInst>(memInst);
  while (true) {
    // Use MemoryDependenceAnalysis to find out
    // DEBUG(errs() << "getDependency for " << *inst << "\n");
    MemDepResult dep = MD->getDependency(inst);

    // if cannot decide, conservatively assume depends on self
    if (dep.isUnknown()) {
      depInst = inst;
    }
    else if (dep.isDef() || dep.isClobber()) {
      // found conflict
      depInst = dep.getInst();

      // loads don't interfere with loads, keep looking
      if (IsLoad && isa<LoadInst>(depInst)) {
        assert(dep.isDef() &&
          "getDependency() detects dependency between non must-alias loads");
        inst = depInst;
        continue;
      }
    }
    // else, no dependency in current block
    else
      depInst = NULL;

    break;
  }

  if (depInst) {
    DEBUG(errs() << "    interference found:" << *depInst << "\n");
    int depPos = InstPosMap.getPos(*depInst);
    return depPos;
  }

  return InstPosMapTy::BadPos;
}

// Collect candidate loads and stores from the current block.
void MemCombine::collectLdSts(BasicBlock& BB)
{
  unsigned pos = 0;

  for (BasicBlock::iterator instIt = BB.begin(), instEnd = BB.end();
       instIt != instEnd;
       ++instIt)
  {
    Instruction& inst = *instIt;
    InstPosMap[&inst] = pos++;
    bool isLoad = isa<LoadInst>(&inst);
    bool isStore = isa<StoreInst>(&inst);
    if (!(VectorizeLoads && isLoad) && !(VectorizeStores && isStore))
      continue;

    if (isLoad) {
      if (LoadInst* load = dyn_cast<LoadInst>(&inst)) {
        // don't combine volatile loads
        if (load->isVolatile()) return;
      }
    }
    else {
      if (StoreInst* store = dyn_cast<StoreInst>(&inst)) {
        // don't combine volatile stores
        if (store->isVolatile()) return;
      }
    }

    // TODO_HSA: Temporarily break on group loads and stores
    //           because of alignment issues
    //           Remove it once we will be able to generate 
    //           _align() modifier in HSAIL
    if (AMDOptions::isTargetHSAIL(&BB) && 
        (isLoad || isStore))
    {
      Value* addr = getLdStPointerOperand(inst);
      const Type *basetype = addr->getType();
      const PointerType *baseptype = dyn_cast<PointerType>(basetype);
      assert(baseptype);
      uint32_t addrSpace = baseptype->getAddressSpace();

      #define GROUP_ADDRESS 3
      if (addrSpace == GROUP_ADDRESS) 
        return;
      #undef GROUP_ADDRESS
    }

    CandLdSts.push_back(&inst);
  }
}

// Get the expression for address of the given load or store.
// The expression is composed of a base address, which is the return value
// of the function, a list of scaled symbols, and a constant offset.
inline const Value* MemCombine::getLdStAddrExpr(
  const Instruction& inst, int64_t& offset,
  SmallVectorImpl<VariableGEPIndex>& varIndices)
{
  const Value* addr = getLdStPointerOperand(inst);
  offset = 0;
  const Value* base = DecomposeGEPExpression(addr, offset, varIndices, DL);
  DEBUG(DumpDecomposeGEPExpression(addr, base, offset, varIndices));
  return base;
}

// Get the constant offset between the addresses of the two loads or stores,
// if they have a constant offset.
// Otherwise, return BadConstantOffset
int64_t MemCombine::getAddrConstOffset(const Instruction& inst1,
                                       const Instruction& inst2)
{
  SmallVector<VariableGEPIndex, 4> varIndices1, varIndices2;
  int64_t offset1, offset2;
  const Value* base1 = getLdStAddrExpr(inst1, offset1, varIndices1);
  const Value* base2 = getLdStAddrExpr(inst2, offset2, varIndices2);
  if (base1 != base2) return BadConstantOffset;

  SmallVector<VariableGEPIndex, 4>::const_iterator it1 = varIndices1.begin();
  SmallVector<VariableGEPIndex, 4>::const_iterator end1 = varIndices1.end();
  SmallVector<VariableGEPIndex, 4>::const_iterator it2 = varIndices2.begin();
  SmallVector<VariableGEPIndex, 4>::const_iterator end2 = varIndices2.end();
  for (; it1 != end1 && it2 != end2; ++it1, ++it2) {
    if ((*it1).V != (*it2).V) return BadConstantOffset;
    if ((*it1).Extension != (*it2).Extension) return BadConstantOffset;
    if ((*it1).Scale != (*it2).Scale) return BadConstantOffset;
  }
  if (it1 != end1 || it2 != end2) return BadConstantOffset;

  return offset1 - offset2;
}

// functor for performing std::find_if() below
struct InPair {
  const Instruction& Inst;
  InPair(const Instruction& inst) : Inst(inst) {}
  bool operator()(InstPair& pair) {
    return pair.first == &Inst || pair.second == &Inst;
  }
};

// check if the given instruction is in CandLdStPairs vector
inline bool MemCombine::inCandLdStPairs(const Instruction& inst)
{
  InstPairVecTy::iterator findIt
    = std::find_if(CandLdStPairs.begin(), CandLdStPairs.end(), InPair(inst));
  return findIt != CandLdStPairs.end();
}

// Find all candidate pairs that can be combined.
// Searching the list of loads or stores to find pairs of loads or stores,
// so that for each pair of loads or stores, the distance between their
// addresses equals to the load or store's width, there is no interferences
// between the them, and the lower address of the two is properly aligned.
void MemCombine::findCandPairs()
{
  InstVecTy::iterator it = CandLdSts.begin();
  InstVecTy::iterator end = CandLdSts.end();
  for (; it != end; ++it) {
    Instruction* inst = *it;

    // if already in CandLdStPair, skip it
    if (inCandLdStPairs(*inst)) continue;

    DEBUG(errs() << "Looking for sibling for " << *inst << "\n");

    Value* addr = getLdStPointerOperand(*inst);

    // we only handle vector of certain element types
    const Type *basetype = addr->getType();
    const PointerType *baseptype = dyn_cast<PointerType>(basetype);
    assert(baseptype);
    uint32_t addrSpace = baseptype->getAddressSpace();
    Type *type = baseptype->getElementType();
    assert(type);
    unsigned size = DL->getTypeStoreSize(type);
    Type* et = type;
    if (type->isVectorTy())
      et = dyn_cast<SequentialType>(type)->getElementType();
    assert(et);
    if (!et->isFloatTy() && !et->isDoubleTy() && !et->isIntegerTy()) {
      DEBUG(errs() << "  unhandled type\n");
      continue;
    }

    // combined vector size cannot exceed our limit
    if (size > MaxGeneratedVecWidth) {
      DEBUG(errs() << "  combined width exceed limit\n");
      continue;
    }

    // get the location of the instruction that "inst" has a memory
    // dependence on
    bool isLoad = isa<LoadInst>(inst);
    int pos = InstPosMap.getPos(*inst);
    assert(pos != InstPosMapTy::BadPos && "inst not in InstPosMap");

    InstVecTy::iterator it2 = it;
    for (++it2; it2 != end; ++it2) {
      Instruction* sibling = *it2;

      if (isLoad && !isa<LoadInst>(sibling)) continue;
      if (!isLoad && !isa<StoreInst>(sibling)) continue;

      // if already in CandLdStPair, skip it
      if (inCandLdStPairs(*sibling)) continue;

      // we only combine loads/stores of same type
      // Note: we allow pairs such as <3 x i32> and i32
      Value* sibAddr = getLdStPointerOperand(*sibling);
      Type *sibBaseType = sibAddr->getType();
      PointerType *sibBasePType = cast<PointerType>(sibBaseType);

      // skip if the two loads/stores are not in the same address space
      if (sibBasePType->getAddressSpace() != addrSpace) {
        DEBUG(errs() << "  not in same address space\n");
        continue;
      }

      Type *sibType = sibBasePType->getElementType();
      Type* sibEt = sibType;
      if (sibType->isVectorTy())
        sibEt = dyn_cast<SequentialType>(sibType)->getElementType();
      if (sibEt != et) continue;

      VectorType* vt = getCombinedVecType(type, sibType);
      uint32_t vecSize = DL->getTypeStoreSize(vt);

      // combined vector size cannot exceed our limit
      if (vecSize > MaxGeneratedVecWidth) {
        DEBUG(errs() << "  combined width exceed limit\n");
        continue;
      }

      // combine only if combined vector size is power-of-2
      // to make it easier for downstream phases to break the combined
      // load/store without having to check for alignment.
      if (!isPowerOf2_32(vecSize)) {
        DEBUG(errs() << "  combined width not power-of-2\n");
        continue;
      }

      Instruction* chkInterfInst;
      if (isLoad) {
        // For loads, check if there are interferences that will prevent
        // the lower load from being coalesced with the higher one.
        chkInterfInst = sibling;
      }
      else {
        // For stores, check if there are interferences that will prevent
        // the higher store from being coalesced with the lower one.
        // Insert a temporary clone of the higher store before the lower store
        // to check interference backwards because MemoryDependenceAnalysis
        // does not support forward interference checking.
        // Create the temporary clone for interference check for stores.
        // Cannot reuse the clone for different siblings, because otherwise
        // memory dependency analysis will cache the result. But since the clone
        // will be inserted at different location for different sibling, the
        // cached mem dependency result will be wrong.
        Instruction* clone = inst->clone();
        clone->insertBefore(sibling);
        chkInterfInst = clone;
      }

      DEBUG(errs() << "  looking for interference between currInst and "
                   << *sibling << ":\n");
      int depPos = memInstInterferePos(*chkInterfInst);

      // delete the temporary clone
      if (chkInterfInst != sibling) {
        MD->removeInstruction(chkInterfInst);
        chkInterfInst->eraseFromParent();
        chkInterfInst = NULL;
      }

      if (depPos != InstPosMapTy::BadPos) {
        // interference found, keep looking
        if (pos < depPos) {
          continue;
        }
      }

      // distance == lower load's size?
      int64_t distance = getAddrConstOffset(*inst, *sibling);
      unsigned sibSize = DL->getTypeStoreSize(sibType);
      uint64_t baseSize = distance < 0 ? size : sibSize;
      if (distance == BadConstantOffset ||
          (uint64_t)abs64(distance) != baseSize) {
        DEBUG(errs() << "  distance " << distance
                     << " size " << size << "\n");
        continue;
      }

      // check lower address's alignment
      Instruction& baseInst = distance < 0 ? *inst : *sibling;
      unsigned baseAlignment;
      if (LoadInst *ld = dyn_cast<LoadInst>(&baseInst)) {
        baseAlignment = ld->getAlignment();
      } else {
        StoreInst *st = cast<StoreInst>(&baseInst);
        baseAlignment = st->getAlignment();
      }
      Value* baseAddr = getLdStPointerOperand(baseInst);
      if (baseAlignment < MinAlignment && baseAlignment < vecSize) {
        if (!AlignAnalysis)
          continue;
        uint64_t baseAlignment = AlignAnalysis->getAlignment(*baseAddr);
        if (baseAlignment < MinAlignment && baseAlignment < vecSize) {
          DEBUG(errs() << "  alignment of " << *baseAddr << " is "
                       << baseAlignment << " < vecSize " << vecSize << "\n");
          continue;
        }
      }

      // found pair to be combined
      if (distance > 0) {
        // check if it's ok to hoist baseAddr before inst, as we will be
        // inserting the new instructions before inst.
        if (Instruction* addrInst = dyn_cast<Instruction>(baseAddr)) {
          if (!okToHoist(inst, addrInst)) {
            DEBUG(errs() << "  not ok to hoist baseAddr " << *addrInst
                         << " before " << *inst << "\n");
            continue;
          }
        }
        std::swap(inst, sibling);
      }
      CandLdStPairs.push_back(InstPair(inst, sibling));
      DEBUG(errs() << "  found pair\n\t"
                   << *inst << "\n\t" << *sibling << "\n");
      break;
    }
  }
}

// Returns whether i1 is defined before i2.
// If either of them is not in the InstPosMap, returns true.
bool MemCombine::dominates(const Value& i1, const Value& i2)
{
  const Instruction* inst1 = dyn_cast<Instruction>(&i1);
  const Instruction* inst2 = dyn_cast<Instruction>(&i2);
  if (inst1 == NULL || inst2 == NULL) return true;

  const BasicBlock* BB1 = inst1->getParent();
  const BasicBlock* BB2 = inst2->getParent();
  // we are only changing instruction within a BasicBlock. So assume anything
  // across BasicBlocks are properly dominated.
  if (BB1 != BB2) return true;
  
  int pos1 = InstPosMap.getPos(*inst1);
  if (pos1 == InstPosMapTy::BadPos) return true;

  int pos2 = InstPosMap.getPos(*inst2);
  if (pos2 == InstPosMapTy::BadPos) return true;

  return pos1 < pos2;
}

// Return true if it is conservatively safe to make the value computed by i
// available at the point "anchor", by hoisting i and instructions i depends on
// before anchor
bool MemCombine::okToHoist(const Instruction *anchor, const Instruction *i)
{
  if (!i||!anchor) return false;

  std::vector<const Instruction*> worklist;
  worklist.push_back(i);
  while (!worklist.empty()) {
    const Instruction* inst = worklist.back();
    worklist.pop_back();

    if (dominates(*inst, *anchor)) continue;
    // to be safe, don't hoist a load inst
    if (isa<LoadInst>(inst)) return false;
    assert(!isa<StoreInst>(inst) && "store inst as operand");

    // operands also needs to be hoisted
    User::const_op_iterator I = inst->op_begin();
    User::const_op_iterator E = inst->op_end();
    for (; I != E; ++I) {
      Instruction *op = dyn_cast<Instruction>(*I);
      if (op == NULL) continue;

      worklist.push_back(op);
    }
  }
  return true;
}

struct InstCmp {
  InstPosMapTy InstPosMap;
  InstCmp(const InstPosMapTy& instPosMap) : InstPosMap(instPosMap) {}
  bool operator()(Instruction* i1, Instruction* i2) {
    int pos1 = InstPosMap.getPos(*i1);
    assert(pos1 != InstPosMapTy::BadPos && "inst not in block");
    int pos2 = InstPosMap.getPos(*i2);
    assert(pos2 != InstPosMapTy::BadPos && "inst not in block");
    return pos1 < pos2;
  }
};

// if otherdeps does not dominate anchor, hoist it before anchor
void MemCombine::hoist(Instruction *anchor, Instruction *i)
{
  if (!i||!anchor) return;

  std::vector<Instruction*> worklist;
  if (!dominates(*i, *anchor)) {
    worklist.push_back(i);
  }
  size_t nProcessed = 0;
  while (worklist.size() != nProcessed) {
    Instruction* inst = worklist.back();
    ++nProcessed;

    // operands also needs to be hoisted
    User::op_iterator I = inst->op_begin();
    User::op_iterator E = inst->op_end();
    for (; I != E; ++I) {
      Instruction *op = dyn_cast<Instruction>(*I);
      if (op == NULL) continue;

      if (!dominates(*op, *anchor)) {
        worklist.push_back(op);
      }
    }
  }

  if (worklist.empty()) return;

  std::sort(worklist.begin(), worklist.end(), InstCmp(InstPosMap));
  for (std::vector<Instruction*>::iterator it = worklist.begin(),
       end = worklist.end();
       it != end; ++it) {
    Instruction* inst = *it;
    inst->moveBefore(anchor);
    DEBUG(errs() << "\tmoved " << *inst << " before " << *anchor << "\n");
  }

  // instructions have been moved, need to re-generate InstPosMap
  InstPosMap.clear();
  BasicBlock& BB = *(i->getParent());
  unsigned pos = 0;
  for (BasicBlock::iterator instIt = BB.begin(), instEnd = BB.end();
       instIt != instEnd;
       ++instIt)
  {
    Instruction& inst = *instIt;
    InstPosMap[&inst] = pos++;
  }
}

// Create a CastInst to cast baseInst's pointer operand to
// "pvt" type, which is a pointer to a certain vector type.
// Insert the CastInst before "insertBefore".
// If baseInst's pointer operand does not dominate "insertBefore",
// hoist it before "insertBefore". If it's not ok to hoist, return NULL;
CastInst* MemCombine::castBasePtr(Instruction& baseInst,
                                  PointerType *pvt,
                                  Instruction& insertBefore,
                                  bool isLoad)
{
  // Extract the actual pointer to cast.
  Value* ptr = getLdStPointerOperand(baseInst);
  // If ptr does not dominate insertBefore, hoist it before insertBefore
  if (!dominates(*ptr, insertBefore)) {
    // we have checked before that it is ok to hoist ptr and its operands
    // before insertBefore
    Instruction* ptrInst = dyn_cast<Instruction>(ptr);
    assert(ptrInst && "Value not dominate inst");
    hoist(&insertBefore, ptrInst);
  }

  // cast the pointer into a vector-4 type pointer
  CastInst *ptrToV4Ptr
    = new BitCastInst(ptr, pvt, "arrayidx_v4", &insertBefore);
  return ptrToV4Ptr;
}

// For the given load, create an ExtractElementInst,
// or a ShuffleVectorInst if the original load is a vector load,
// to extract value from vecload at the corresponding index,
// then replace the original load with the new ExtractElementInst
// or ShuffleVectorInst.
void MemCombine::replaceLoadWithExtractElem(LoadInst& vecload,
                                            VectorType* vt,
                                            unsigned startIdx,
                                            LoadInst& load,
                                            Type* loadType)
{
  IntegerType *int32ty = Type::getInt32Ty(vecload.getContext());

  Instruction* extractElem = NULL;
  if (loadType->isVectorTy()) {
    SmallVector<Constant*,4> indices;
    unsigned numElem = dyn_cast<VectorType>(loadType)->getNumElements();
    for (unsigned i = 0; i < numElem; ++i) {
      indices.push_back(ConstantInt::get(int32ty, startIdx + i));
    }
    Constant* mask = ConstantVector::get(indices);
    extractElem = new ShuffleVectorInst(&vecload, UndefValue::get(vt), mask);
    startIdx += numElem;
  }
  else {
    Value* idx = ConstantInt::get(int32ty, startIdx);
    extractElem = ExtractElementInst::Create(&vecload, idx);
    ++startIdx;
  }
  extractElem->insertBefore(&load);
  MD->removeInstruction(&load);
  BasicBlock::iterator BI(&load);
  ReplaceInstWithValue(load.getParent()->getInstList(), BI, extractElem);
}

// For the given pair of stores, create a ShuffleVectorInst to combine the
// two values to be stored, and replace the original stores
// with the ShuffleVectorInst.
// Note: The two stores must have the same type
// Return the ShuffleVec instruction created.
Value& MemCombine::replaceStoresWithShuffleVec(Type* type,
                                               InstPair storePair,
                                               Instruction& insertBefore)
{
  IntegerType *int32ty = Type::getInt32Ty(type->getContext());

  // form a local vector with the values to be stored by each store
  Value* values[2];
  // first get each stored values
  for (unsigned i = 0; i < 2; ++i) {
    Instruction* inst = i == 0 ? storePair.first : storePair.second;
    StoreInst* store = dyn_cast<StoreInst>(inst);
    Value* val = store->getValueOperand();
    if (!type->isVectorTy()) {
      // cast the scalar value into a vector type
      VectorType *vt = VectorType::get(type, 1);
      values[i] = new BitCastInst(val, vt, "val2vec", &insertBefore);
    }
    else
      values[i] = val;
    MD->removeInstruction(store);
    store->eraseFromParent();
  }
  // combine stored values into one value. e.g.:
  // v1, v2 of type <4 x i8>
  // ==> v1' = shuffle v1, v2, <8 x i8><0,1,2,3,4,5,6,7>
  Value* v1 = values[0];
  Value* v2 = values[1];
  assert(v1->getType() == v2->getType() && "v1 and v2 has diff type");
  assert(v1->getType()->isVectorTy() && "v1 not vector type");
  unsigned numElem
    = dyn_cast<VectorType>(v1->getType())->getNumElements();
  SmallVector<Constant*,4> indices;
  for (unsigned i = 0; i < 2 * numElem; ++i) {
    indices.push_back(ConstantInt::get(int32ty, i));
  }
  Constant* mask = ConstantVector::get(indices);
  ShuffleVectorInst* shuffle = new ShuffleVectorInst(v1, v2, mask);
  shuffle->insertBefore(&insertBefore);
  return *shuffle;
}

// For the given store, create a InsertElement to insert the value to be stored
// by "store" into "vec", then remove the "store" instruction.
// Return the InsertElem instruction created.
Value& MemCombine::replaceStoreWithInsertElem(
  StoreInst& store,
  Value *vec,
  size_t idx,
  Instruction& insertBefore)
{
  IntegerType *int32ty = Type::getInt32Ty(store.getContext());
  Value* val = store.getValueOperand();
  Value* idxConst = ConstantInt::get(int32ty, idx);
  Instruction* insertElem = InsertElementInst::Create(vec, val, idxConst);
  insertElem->insertBefore(&insertBefore);
  MD->removeInstruction(&store);
  store.eraseFromParent();
  return *insertElem;
}

// For the given store which is storing a vector, create a ShuffleVec
// to insert the vector of values to be stored by "store" into a vector,
// then remove the "store" instruction.
// Return the ShuffleVec instruction created.
Value& MemCombine::replaceStoreWithShuffleVec(
  Type* vecType,
  size_t startIdx,
  StoreInst& store,
  Type* storeType,
  Instruction& insertBefore)
{
  IntegerType *int32ty = Type::getInt32Ty(store.getContext());

  assert(storeType->isVectorTy() && "should use InsertElement instead");

  SmallVector<Constant*,4> indices;
  unsigned vecNumElem = dyn_cast<VectorType>(vecType)->getNumElements();
  unsigned storeNumElem = dyn_cast<VectorType>(storeType)->getNumElements();
  for (unsigned i = 0; i < vecNumElem; ++i) {
    if (i < startIdx || i >= startIdx + storeNumElem)
      indices.push_back(UndefValue::get(int32ty));
    else
      indices.push_back(ConstantInt::get(int32ty, i-startIdx));
  }
  Constant* mask = ConstantVector::get(indices);
  Value* val = store.getValueOperand();
  Instruction* shuffle
    = new ShuffleVectorInst(val, UndefValue::get(storeType), mask);
  shuffle->insertBefore(&insertBefore);
  MD->removeInstruction(&store);
  store.eraseFromParent();
  return *shuffle;
}

// For the pair of stores, create ShuffleVec's or InsertElem's
// to insert the values to be stored by the pair of store into a vector,
// then remove the stores.
// Return the last new instruction created which represents the final
// vector that holds all the values to be stored by the pair of stores.
Value& MemCombine::replaceStoresWithShuffleVecOrInsertElem(
  Type* type1,
  Type* type2,
  VectorType *vt,
  InstPair storePair,
  Instruction& insertBefore)
{
  if (type1 == type2 && type1->isVectorTy()) {
    return replaceStoresWithShuffleVec(type1, storePair, insertBefore);
  }

  if (!type1->isVectorTy() && !type2->isVectorTy()) {
    Value* localVec = UndefValue::get(vt);
    StoreInst* store = dyn_cast<StoreInst>(storePair.first);
    localVec = &replaceStoreWithInsertElem(*store,
                                           localVec,
                                           0,
                                           insertBefore);
    store = dyn_cast<StoreInst>(storePair.second);
    localVec = &replaceStoreWithInsertElem(*store,
                                           localVec,
                                           1,
                                           insertBefore);
    return *localVec;
  }

  if (type1->isVectorTy() && type2->isVectorTy()) {
    StoreInst* store = dyn_cast<StoreInst>(storePair.first);
    IntegerType *int32ty = Type::getInt32Ty(store->getContext());
    Value& vec1 = replaceStoreWithShuffleVec(vt,
                                             0,
                                             *store,
                                             type1,
                                             insertBefore);
    unsigned nextStartIdx = dyn_cast<VectorType>(type1)->getNumElements();
    store = dyn_cast<StoreInst>(storePair.second);
    Value& vec2 = replaceStoreWithShuffleVec(vt,
                                             nextStartIdx,
                                             *store,
                                             type2,
                                             insertBefore);
    // create another ShuffleVector to combine the two vectors
    SmallVector<Constant*,4> indices;
    unsigned numElem = dyn_cast<VectorType>(vt)->getNumElements();
    for (unsigned i = 0; i < numElem; ++i) {
      indices.push_back(ConstantInt::get(int32ty, i));
    }
    Constant* mask = ConstantVector::get(indices);
    Instruction* shuffle = new ShuffleVectorInst(&vec1, &vec2, mask);
    shuffle->insertBefore(&insertBefore);
    return *shuffle;
  }

  StoreInst* store1 = dyn_cast<StoreInst>(storePair.first);
  StoreInst* store2 = dyn_cast<StoreInst>(storePair.second);
  if (!type1->isVectorTy()) {
    std::swap(store1, store2);
    std::swap(type1, type2);
  }

  Value* localVec = &replaceStoreWithShuffleVec(vt,
                                                0,
                                                *store1,
                                                type1,
                                                insertBefore);
  size_t nextStartIdx = dyn_cast<VectorType>(type1)->getNumElements();
  localVec = &replaceStoreWithInsertElem(*store2,
                                         localVec,
                                         nextStartIdx,
                                         insertBefore);
  return *localVec;
}

// Return a vector type that is a combine of the two input types
VectorType* MemCombine::getCombinedVecType(Type* type1,
                                           Type* type2)
{
  Type* et = NULL;
  unsigned numElem = 0;

  for (size_t i = 0; i < 2; ++i) {
    Type* t = i == 0 ? type1 : type2;
    Type* et2 = NULL;
    if (t->isVectorTy()) {
      et2 = dyn_cast<SequentialType>(t)->getElementType();
      numElem += dyn_cast<VectorType>(t)->getNumElements();
    }
    else {
      et2 = t;
      ++numElem;
    }
    if (i == 0) {
      et = et2;
    }
    else {
      assert(et == et2 && "vector elements with different types");
    }
  }

  return VectorType::get(et, numElem);
}

// Combine the pair of loads or stores into one vector load or store.
bool MemCombine::combinePair(InstPair pair)
{
  DEBUG(errs() << "MemCombine::combinePair\n\t"
               << *pair.first << "\n\t" << *pair.second << "\n");

  Value* ptrBase = getLdStPointerOperand(*pair.first);
  const PointerType *baseptype = dyn_cast<PointerType>(ptrBase->getType());
  assert(baseptype);
  Type *type1 = baseptype->getElementType();
  assert(type1);

  const Value* ptrBase2 = getLdStPointerOperand(*pair.second);
  PointerType *baseptype2 = dyn_cast<PointerType>(ptrBase2->getType());
  assert(baseptype2);
  Type *type2 = baseptype2->getElementType();
  assert(type2);

  VectorType* vt = getCombinedVecType(type1, type2);

  // <2 x element-type> *
  PointerType *pvt = PointerType::get(vt, baseptype->getAddressSpace());
  DEBUG(errs() << "[combinePair]  Coalescing accesses as "
               << *vt << " memory accesses. \n");

  // vector instructions will be inserted before the top load/bottom store
  int pos1 = InstPosMap.getPos(*pair.first);
  assert(pos1 != InstPosMapTy::BadPos && "inst not in InstPosMap");
  int pos2 = InstPosMap.getPos(*pair.second);
  assert(pos2 != InstPosMapTy::BadPos && "inst not in InstPosMap");
  Instruction* baseInst = pair.first;
  Instruction *insertBefore;
  uint32_t vecAlign;
  bool isLoad = isa<LoadInst>(baseInst);
  if (isLoad) {
    insertBefore = pos1 < pos2 ? pair.first : pair.second;
    LoadInst *ld = cast<LoadInst>(baseInst);
    vecAlign = ld->getAlignment();
  }
  else {
    insertBefore = pos1 < pos2 ? pair.second : pair.first;
    StoreInst *st = cast<StoreInst>(baseInst);
    vecAlign = st->getAlignment();
  }
  if (AlignAnalysis) {
    uint64_t align2 = AlignAnalysis->getAlignment(*ptrBase);
    if (align2 > vecAlign)
      vecAlign = align2;
  }
  assert(isPowerOf2_32(vecAlign) && "combined vector size not power-of-2");

  // insert BitCast to cast original pointer to dest vector pointer
  CastInst *ptrToVecPtr = castBasePtr(*baseInst, pvt, *insertBefore, isLoad);
  if (!ptrToVecPtr) return false;
  DEBUG(errs() << "[combinePair]  Casting base address with"
               << *ptrToVecPtr << "\n");

  if (isLoad) {
    // Locate the first load and insert a vector load of length 2
    // before the first scalar load using base GEPI
    LoadInst *vecload = NULL;
    // insert the vector load before 1st scalar load in the isomorphic
    // instruction groups
    vecload = new LoadInst(ptrToVecPtr, "vecload", insertBefore);
    vecload->setAlignment(vecAlign);
    DEBUG(errs() << "[combinePair]  Insert vector load \n\t" << *vecload
                 << " before \n\t" << *insertBefore << "\n");

    LoadInst* load = dyn_cast<LoadInst>(pair.first);
    replaceLoadWithExtractElem(*vecload, vt, 0, *load, type1);
    unsigned nextStartIdx = type1->isVectorTy()
                            ? dyn_cast<VectorType>(type1)->getNumElements()
                            : 1;
    load = dyn_cast<LoadInst>(pair.second);
    replaceLoadWithExtractElem(*vecload, vt, nextStartIdx, *load, type2);
  }
  else {
    Value& localVec
      = replaceStoresWithShuffleVecOrInsertElem(type1, type2, vt, pair,
                                                *ptrToVecPtr);

    // create a vector store that stores the values in the local vector
    // to the vector specified by the new GEPI we created
    StoreInst* vecstore = new StoreInst(&localVec, ptrToVecPtr);
    vecstore->setAlignment(vecAlign);
    vecstore->insertAfter(ptrToVecPtr);
    DEBUG(errs() << "[combinePair]  Insert vector store \n\t" << *vecstore
                 << " after \n\t" << *ptrToVecPtr << "\n");
  }
  return true;
}

// Try to combine all load/stores to memory siblings that's eligible
bool MemCombine::combineMemSiblings(BasicBlock& BB)
{
  InstPosMap.clear();
  CandLdSts.clear();
  CandLdStPairs.clear();

  // MDA does not preserve correct information even after 
  // removeInstruction calls. It can lead to incorrect
  // results here and in later passes. 
  // That caused failures on some of the HSA tests.
  MD->releaseMemory();

  // Collect the candidate loads and stores in the current region.
  collectLdSts(BB);

  // Find all candidate pairs that can be combined.
  findCandPairs();

  bool changed = false;
  {
    // combine all the candidate pairs found
    InstPairVecTy::iterator it = CandLdStPairs.begin();
    InstPairVecTy::iterator end = CandLdStPairs.end();
    for (; it != end; ++it) {
      InstPair pair = *it;
      changed |= combinePair(pair);
    }
  }

  DEBUG(errs() << "IR Dump after one iteration:\n" << BB);
  return changed;
}

MemCombine::MemCombine(unsigned maxVecGen, unsigned minAlign)
  : BasicBlockPass(ID), AlignAnalysis(NULL), MD(NULL), DL(NULL),
    MaxGeneratedVecWidth(maxVecGen), MinAlignment(minAlign),
    CandLdSts(), CandLdStPairs(), InstPosMap()
{
  initializeMemCombinePass(*PassRegistry::getPassRegistry());

  // if MaxGeneratedVecWidth is not power-of-2, round it to the
  // next power-of-2 that is smaller than its current value
  if (!isPowerOf2_32((uint32_t)MaxGeneratedVecWidth))
    MaxGeneratedVecWidth = Log2_32((uint32_t)MaxGeneratedVecWidth);
  // if MinAlignment is not power-of-2, round it to the
  // next power-of-2 that is smaller than its current value
  if (!isPowerOf2_32((uint32_t)MinAlignment))
    MinAlignment = Log2_32((uint32_t)MinAlignment);
}

// driver of this class
// This is the main transformation entry point for a basic block.
bool MemCombine::runOnBasicBlock(BasicBlock &BB) {
  const OpenCLSymbols &OCLS = getAnalysis<OpenCLSymbols>();

  // We can only assume kernels having aligned pointer-typed parameters.
  // Also, skip stubs
  Function* F = BB.getParent();
  if (!(OCLS.isKernel(F) || OCLS.isStub(F)))
    return false;

  DEBUG(errs() << "Function:\n");
  DEBUG(errs().write_escaped(F->getName()) << '\n');

  MD = &getAnalysis<MemoryDependenceAnalysis>();
  AlignAnalysis = getAnalysisIfAvailable<AlignmentAnalysis>();
  DL = getAnalysisIfAvailable<DataLayout>();
  if (DL == NULL) {
    DEBUG(errs() << "runOnBasicBlock: target data not available\n");
    return false;
  }

  // keep trying until nothing can be combined
  bool ret_flag = false;

  for (unsigned i = 0; i < MaxGeneratedVecWidth; ++i) {
    bool changed = combineMemSiblings(BB);
    if (!changed) break;

    ret_flag = true;
  }

  return ret_flag;
}

// createAMDMemCombinePass - The public interface to this file...
BasicBlockPass *llvm::createAMDMemCombinePass(unsigned maxVecGen,
                                              unsigned minAlign)
{
  return new MemCombine(maxVecGen, minAlign);
}

static void DumpDecomposeGEPExpression(
  const Value *V, const Value* base,
  int64_t BaseOffs,
  SmallVectorImpl<VariableGEPIndex> &VarIndices)
{
  dbgs() << "  decomposed " << *V << " is: \n";
  dbgs() << "    base " << *base << " offset " << BaseOffs << " vars\n";
  SmallVectorImpl<VariableGEPIndex>::const_iterator it = VarIndices.begin();
  SmallVectorImpl<VariableGEPIndex>::const_iterator end = VarIndices.end();
  for (; it != end; ++it) {
    errs() << "\t" << (*it).Scale << " * " << *(*it).V << "\n";
  }
}

#endif // 1 || defined (AMD_OPENCL_MEMCOMBINE)
